I.) Overview
II.) Requirements
   1.) ssh requirements
   2.) sudo requirements
   3.) http proxy

I.) Overview

autobuild is carried out in 3 phases

Phase 1: installing the development KATA system

Phase 2: packaging the development KATA system (i.e. builing prod.rpm)
         and transferring all 3 rpms to the control host

Phase 3: installing the production system


Phase 1 + 2 are carried out on the development host. The phases have been
separated because after phase 1 we have a fully functional KATA system.
It might be useful to do some (smoke) testing between these phases. (Once
we have that fully automated the phases can be combined.)

Phase 3 is carried out on the production host. There are 2 choices:
Either the VM used as development machine before is reset and used
again or there is a dedicated production host. Because we haven't
scripted VMs yet, this needs to be a separate phase.

The phase is given as a numeric argument to the autobuild script

$ autobuild/autobuild.sh [1|2|3]

It is possible to have development machine and production machine in the same
VM, if you reset the machine to a clean installation between phase 2 & 3


II.) Requirements

1.) ssh requirements

Host and user names can be found from autobuild.sh (they are actually
configurable, but this has not been tested)

A (private) ssh key is used to access the buildhost, it's public
counterpart must be authorized for the autobuild user. The local
location is configurable (as before)

Historical note: In some phase we also required ssh access to root@buildhost.
This has been replaced by sudo as below, because 

1.) it makes use of the existing setup-buildenv.sh easier
2.) we don't need to copy the private key to the build host (e.g.
    if we modified setup@buildenv.sh to do a ssh root@localhost in case
    there is no tty)


2.) sudo requirements
 
On the buildhost the build user needs sudo access to root without password and
required tty for the following 7 commands

yum: for obvious reasons, installation is what we do here
cp: to be able to copy shibboleth repo definition to /etc/yum/repos.d
service: to be able to stop kata related services before building prod.rpm
true: prod.spec calls this to force the password prompt in interactive usage
find: called by prod.spec when packaging CKAN files
cpio: as find
chown: as find

Acutally on the production host yum should be enough (this has not been
tested because I use a common template for both hosts)

#
# Disable "ssh hostname sudo <cmd>", because it will show the password in clear.
#         You have to run "ssh -t hostname sudo <cmd>".
#
Defaults    requiretty
Defaults:abuild !requiretty     #last match used

...

abuild  ALL=(root)   NOPASSWD: /bin/cp, /usr/bin/yum, /sbin/service, /bin/true, /bin/find, /bin/cpio, /bin/chown



3.) http proxy

It might be useful to use a local http proxy, because a lot of packages
are downloaded from the net repeatedly.

For wget the proxy can be set in /etc/environment like this

http_proxy=http://192.168.56.68:80


yum doens't seem to obey this environment variable, it needs its own
configuration in /etc/yum.conf like this

proxy=http://192.168.56.68:80


Note: Apache disk cache file size is limited to 1 MB by default.
      http://httpd.apache.org/docs/2.2/mod/mod_disk_cache.html

      E.g. Solr source tar ball is about 70 MB, so I changed the limit to
      100 MB like this

      CacheMaxFileSize 100000000
